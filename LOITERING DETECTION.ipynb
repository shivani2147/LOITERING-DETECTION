{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd006b3",
   "metadata": {},
   "source": [
    "# INTRODUCTION \n",
    "\n",
    "Loitering detection is a surveillance technology designed to identify and alert to suspicious or prolonged presence in a specific area. . Typically used in security and public safety applications, loitering detection systems employ sensors, cameras, or computer vision algorithms to monitor a defined space. It uses sensors, cameras, or computer vision to trigger alerts when something remains stationary for too long. It plays a crucial role in enhancing security, deterring potential threats, and maintaining public safety in various environments, including retail stores, transportation hubs, and public spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b775b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import cvzone\n",
    "import math\n",
    "from sort.sort import Sort\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04a7ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture the Video \n",
    "cap = cv2.VideoCapture(\"D:\\Videos\\cctv_.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02ea6960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Model of YOLOv8\n",
    "model = YOLO('yolov8l.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b221c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Names of the different classes\n",
    "\n",
    "classNames = ['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', \n",
    "              'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', \n",
    "              'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', \n",
    "              'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', \n",
    "              'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', \n",
    "              'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'potted plant', 'bed', 'dining table', 'toilet', \n",
    "              'tv monitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', \n",
    "              'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda1d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Sort (for tracking the Id)\n",
    "tracker = Sort(max_age = 25, min_hits = 3, iou_threshold = 0.5)\n",
    "          # max_age : maximum no. of frames that an object can be tracked without being detected\n",
    "          # min_hits : minimum number of detections required to create a track\n",
    "          # iou_threshold : sets the minimum IoU (intersection over union) overlap required between two bounding boxes to consider them as the same object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8d0ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_dict = {}         # Dictionary to store tracking information for objects\n",
    "\n",
    "blink_text = True          # To make the \"Loitering..!\" text blink \n",
    "frame_count = 0            # Counter for frames to control blinking\n",
    "\n",
    "captured_first_frame = {}            # Dictionary to track if the first frame for each object is captured\n",
    "is_loitering = False                 # Flag to indicate if someone is currently loitering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28216769",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True : \n",
    "    # Read a frame from the video capture\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Checks whether the frame was read successfully or not\n",
    "    if not ret : \n",
    "        break\n",
    "        \n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "    \n",
    "    # Use the model to detect objects in the current frame and enable streaming display\n",
    "    results = model(frame, stream = True)\n",
    "              #stream : function will treat the input as a continuous stream of frames, such as a video\n",
    "    \n",
    "    # Initialize an empty array to store detected object information\n",
    "    detections = np.empty((0, 5))           #(0 rows & 5 cols) to store the bounding box coordinates and its conf\n",
    "    \n",
    "    # To extract info from the results of obj detection \n",
    "    for r in results : \n",
    "        boxes = r.boxes        # Get the bounding boxes of detected objects in this result\n",
    "        for box in boxes : \n",
    "            # Extract the coordinates of the bounding box\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)     # Convert the coordinates to integers\n",
    "            # Calculate and round the confidence score / predicted accuracy\n",
    "            conf = math.ceil((box.conf[0] * 100)) / 100\n",
    "            # Get the class label index of the detected object\n",
    "            cls = int(box.cls[0])\n",
    "            # Name of the detected class using the index\n",
    "            currentClass = classNames[cls]\n",
    "            \n",
    "            if currentClass == 'person' : \n",
    "                # Create an array with object information\n",
    "                currentArray = np.array([x1, y1, x2, y2, conf])\n",
    "                # To stack vertically (means to append each detections vertically)\n",
    "                detections = np.vstack((detections, currentArray))\n",
    "    \n",
    "    # Update tracker with the new bounding box (list of detections)\n",
    "    resultsTracker = tracker.update(detections)\n",
    "        #The purpose of this line is to update the object tracker with the latest information about the detected \"person\" objects.\n",
    "    \n",
    "    # Tracked results\n",
    "    for res in resultsTracker : \n",
    "        # Extract the coordinates and ID of the tracked object\n",
    "        x1, y1, x2, y2, Id = res\n",
    "        x1, y1, x2, y2, Id = int(x1), int(y1), int(x2), int(y2), int(Id)    \n",
    "        # Calculate width and height of the bounding box\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        \n",
    "        # If the object ID is not in the tracker dictionary, initialize its data\n",
    "        if Id not in tracker_dict : \n",
    "            tracker_dict[Id] = {'bbox' : None, 'Center' : []}\n",
    "            \n",
    "        # Store the bounding box coordinates\n",
    "        tracker_dict[Id]['bbox'] = [x1, y1, w, h]\n",
    "        \n",
    "        # Calculate center coordinates (center points) of the object\n",
    "        center_points = x1 + w // 2, y1 + h // 2\n",
    "        # Append the center coordinates\n",
    "        tracker_dict[Id]['Center'].append(center_points)\n",
    "        \n",
    "        if Id in tracker_dict : \n",
    "            # Retrieve the bounding box\n",
    "            bbox = tracker_dict[Id]['bbox']\n",
    "        # Draw a corner rectangle around the object's bounding box\n",
    "        cvzone.cornerRect(frame, bbox, l = 8, rt = 1, colorR = (255, 255, 50))\n",
    "        # Display the object's ID near the top-left corner of its bounding box\n",
    "        cv2.putText(frame, f\"ID : {Id}\", (max(0, bbox[0]), max(0, bbox[1] - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Check if 'traced_path' exists in tracker_dict for the current object (Id)\n",
    "        if 'traced_path' not in tracker_dict[Id] : \n",
    "            # If not, initialize 'traced_path' with the current center point\n",
    "            tracker_dict[Id]['traced_path'] = [center_points]\n",
    "        else : \n",
    "            # If already exists, add the current center point to the traced path\n",
    "            tracker_dict[Id]['traced_path'].append(center_points)\n",
    "\n",
    "        # Get the traced path of the current object\n",
    "        tracked_path = tracker_dict[Id]['traced_path']\n",
    "        # Draw lines between consecutive traced path points to visualize the path\n",
    "        for i in range(1, len(tracked_path)) :        # It loops through all the center points\n",
    "            cv2.line(frame, tracked_path[i - 1], tracked_path[i], (0, 255, 255), 1)\n",
    "                # Draws a line between the previous center point and the current center point\n",
    "    \n",
    "        if 'variance' not in tracker_dict[Id] : \n",
    "            # If not, initialize 'variance' with 0\n",
    "            tracker_dict[Id]['variance'] = 0\n",
    "            \n",
    "        # Check if there are recorded movement center points for the current object\n",
    "        if len(tracker_dict[Id]['Center']) > 0 : \n",
    "            # Convert the recorded center points to an array\n",
    "            points = np.array(tracker_dict[Id]['Center'])\n",
    "            \n",
    "            var_x = np.var(points[:, 0])         # Variance of x-coordinates to measure horizontal movement\n",
    "            var_y = np.var(points[:, 1])         # Variance of y-coordinates to measure vertical movement\n",
    "            var = (var_x + var_y) / 2         # Calculate average variance by combining horizontal and vertical variances\n",
    "            \n",
    "            if var > 2000 : \n",
    "                tracker_dict[Id]['variance'] = var\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "                cv2.putText(frame, f\"Var : {round(tracker_dict[Id]['variance'], 2)}\", (x1, y1 - 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            \n",
    "                # After every 15 frames, it will toggle the \"Loitering..!\" text\n",
    "                if frame_count % 15 == 0 : \n",
    "                    blink_text = not blink_text\n",
    "                frame_count += 1      # Increment the frame count\n",
    "                \n",
    "                # Display a warning text for potential loitering\n",
    "                if blink_text : \n",
    "                    cv2.putText(frame, f'Loitering..!', (30, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "                    \n",
    "                # Check if the first frame of loitering is captured for this object ID\n",
    "                if not captured_first_frame.get(Id, False) : \n",
    "                    loitering_start_time = datetime.now()       # Get the current time as the start time of loitering\n",
    "                    captured_first_frame[Id] = True          # Mark that the first frame has been captured\n",
    "                    \n",
    "                    # Get the current time and format \n",
    "                    timestamp = loitering_start_time.strftime('%d/%m/%Y - %H:%M:%S')\n",
    "                    loitering_text = f'Loitering Start : {timestamp}'\n",
    "                    # Display the loitering start text on the frame\n",
    "                    cvzone.putTextRect(frame, loitering_text, (max(30, 10), max(450, 35)), font = cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                       scale = 0.5,  thickness = 1, colorR = (50, 50, 255))\n",
    "    \n",
    "                    # Save the current frame as an image\n",
    "                    img_file = f'Loitering_ID {Id}.jpg'\n",
    "                    cv2.imwrite(img_file, frame)\n",
    "                    \n",
    "            # If variance is below the threshold\n",
    "            else : \n",
    "                # Check if loitering was previously detected\n",
    "                if is_loitering : \n",
    "                    # Get the current time as the end time of loitering\n",
    "                    loitering_end_time = datetime.now()\n",
    "                    # Reset the is_loitering \n",
    "                    is_loitering = False\n",
    "                    # Calculate the duration of loitering\n",
    "                    loitering_duration = (loitering_end_time - loitering_start_time).seconds\n",
    "                    # Display loitering end time text on the frame\n",
    "                    cvzone.putTextRect(frame, loitering_text, (max(30, 10), max(450, 35)), font = cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                       scale = 0.5,  thickness = 1, colorR = (50, 50, 255))\n",
    "    \n",
    "    # Display the Video\n",
    "    cv2.imshow(\"Loitering Detection\", frame)\n",
    "    if cv2.waitKey(1) == ord('q') : \n",
    "        break\n",
    "        \n",
    "# Release the video capture and close OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f94f8e0",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- It initializes the YOLO model for object detection using 'yolov8l.pt' and defines a list of Class Names that the model can detect.\n",
    "- A SORT (Simple Online and Realtime Tracking) tracker is initialized with specific parameters.\n",
    "- Several dictionaries and variables are set up to keep track of detected objects, loitering status, and other information.\n",
    "- The code enters a loop to process frames from a video capture.\n",
    "- Detected object information, including bounding box coordinates and confidence scores, is extracted and stored in an array.\n",
    "- The SORT tracker is updated with the detected objects' information.\n",
    "- Tracked objects are visualized on the frame with bounding boxes, IDs, and paths.\n",
    "- The code calculates the variance in the movement of tracked objects. High variance suggests potential loitering.\n",
    "- If potential loitering is detected, it captures the frame, displays a warning, and records the start time of loitering.\n",
    "- If the variance decreases below a threshold, it marks the end of loitering and records the end time.\n",
    "- The processed frame with loitering detection information is displayed.\n",
    "- Finally, it releases the video capture and closes any open windows.\n",
    "---\n",
    "This code aims to identify and track objects, particularly people, in a video stream and detect potential loitering behavior based on the objects' movements and variance in their positions over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd185e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YOLO",
   "language": "python",
   "name": "yolo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
